{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMT7Nmh+NZ5sR0AoDRF/1AH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarmi2325/Neural_Network/blob/main/06_MLP_from_Scratch_Multiclass_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "ak1pGhAzIIF4",
        "outputId": "797c762d-27b9-47fe-f139-d21f0b6bcf6a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x300 with 5 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAACZCAYAAABHTieHAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAB0NJREFUeJzt3TFSU38Xx+H4Dj2JK2BwAWYm9qaQWiywNVaU0EFnylBJi1WoUxhqMiMLkJm4gQwrMGEFvFuQ//eegOZ5an/nnoSYy2duwYuHh4eHFgAAQMP+99QLAAAA/yaxAQAAlBAbAABACbEBAACUEBsAAEAJsQEAAJQQGwAAQAmxAQAAlNj603/44sWLyj3+yMHBQTxjNBrFM2azWTzj9PQ0Or9cLuMdmrCuvwn5HD5/Tbi5uYlntNvteMZwOIzOT6fTeIcmrPNvkv4rn8F+vx/PaOLnP5/Po/NNvI4mbNJ34MnJSTyjiXvwYrGIZ7x58yY67x78d2ri/jkej+MZ+/v78Yzn4E8/f55sAAAAJcQGAABQQmwAAAAlxAYAAFBCbAAAACXEBgAAUEJsAAAAJcQGAABQQmwAAAAlxAYAAFBCbAAAACXEBgAAUEJsAAAAJcQGAABQQmwAAAAlxAYAAFBi66kXeIzRaBTP2N3djWd0Op14xu/fv6PzHz9+jHeYTCbxDB5ntVrFM96+fRvP6Pf70fnpdBrvwON1u914xo8fP+IZ9/f38YydnZ14Bo+T3kMPDg7iHQ4PD+MZFxcX8Yxerxedn81m8Q6s32AwiGfM5/N4xqbxZAMAACghNgAAgBJiAwAAKCE2AACAEmIDAAAoITYAAIASYgMAACghNgAAgBJiAwAAKCE2AACAEmIDAAAoITYAAIASYgMAACghNgAAgBJiAwAAKLG1zov1er3o/O7ubrzDq1ev4hmLxSKecX19HZ1P38tWq9WaTCbxjE3S7XbjGf1+P57RhPl8/tQr8B/s7+/HM379+hXPmE6n8YwvX77EM3icb9++RefPzs7iHX7+/BnPaOIePJvN4hmsV7vdjmcMBoN4xvn5eTxjZ2cnnpG6u7tb27U82QAAAEqIDQAAoITYAAAASogNAACghNgAAABKiA0AAKCE2AAAAEqIDQAAoITYAAAASogNAACghNgAAABKiA0AAKCE2AAAAEqIDQAAoITYAAAASogNAACgxNY6L9bpdKLzt7e38Q6LxSKe0YQmXguPc3x8HJ0fDofxDtvb2/GMJtzc3Dz1CvwH5+fn8Yy7u7tnscfV1VU8g8dJ73+7u7vxDk3MmM1m8Yz095HlchnvwOMMBoN4xs7OTjxjPB7HM9Lv0NVqFe/QxO80f8qTDQAAoITYAAAASogNAACghNgAAABKiA0AAKCE2AAAAEqIDQAAoITYAAAASogNAACghNgAAABKiA0AAKCE2AAAAEqIDQAAoITYAAAASogNAACghNgAAABKbK3zYp1OJzo/m80a2uTppe/FcrlsaJPNcX5+Hp0fj8fxDs/l59Zut596hY2Uvu/Hx8fxDvv7+/GMJgwGg6degUdaLBbxjJcvX8Yzrq+vn3zG3t5evMNzuR+sS/rd8/Xr13iHy8vLeEYTjo6OovOfP39uaJP18GQDAAAoITYAAIASYgMAACghNgAAgBJiAwAAKCE2AACAEmIDAAAoITYAAIASYgMAACghNgAAgBJiAwAAKCE2AACAEmIDAAAoITYAAIASYgMAACghNgAAgBJb67zYcrmMzvd6vYY2yXQ6nXhG+lomk0m8A5ur2+1G5+fzeSN7bJrhcBidPzo6amaR0IcPH+IZq9UqX4S/Tvp7QKvVau3t7cUzLi4uovMnJyfxDqenp/GMv0n6f/7+/j7e4dOnT/GM9P7ZhOl0+tQrPIonGwAAQAmxAQAAlBAbAABACbEBAACUEBsAAEAJsQEAAJQQGwAAQAmxAQAAlBAbAABACbEBAACUEBsAAEAJsQEAAJQQGwAAQAmxAQAAlBAbAABAia11XmyxWETne71evMPBwcGzmJE6Ozt76hWARxqPx9H5fr8f7/D69et4xvfv3+MZV1dX0fn0vWy1Wq3pdBrP2CSj0SieMZvN4hmdTiee8e7du+j8ZDKJd9g0Nzc30fl2ux3v0O124xnp62i1Wq3Ly8vo/Gq1indYJ082AACAEmIDAAAoITYAAIASYgMAACghNgAAgBJiAwAAKCE2AACAEmIDAAAoITYAAIASYgMAACghNgAAgBJiAwAAKCE2AACAEmIDAAAoITYAAIASYgMAACixtc6LLRaL6Pzp6Wm8w2g0imfc3t7GM968eRPPYL1Wq1U84+rqKp7x/v37eEa/34/Oj8fjeIdNNJ/Po/PdbjfeoYkZw+EwnpF+ju/u7uIdptNpPGOTLJfLeMbFxUUDm+Qmk0l0/vDwsKFNWKcm7uPb29vxjE27h3qyAQAAlBAbAABACbEBAACUEBsAAEAJsQEAAJQQGwAAQAmxAQAAlBAbAABACbEBAACUEBsAAEAJsQEAAJQQGwAAQAmxAQAAlBAbAABACbEBAACUEBsAAECJFw8PDw9PvQQAAPDv8WQDAAAoITYAAIASYgMAACghNgAAgBJiAwAAKCE2AACAEmIDAAAoITYAAIASYgMAACjxf1PjMlpt/v6HAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "images=load_digits()\n",
        "X=images.data\n",
        "y=images.target\n",
        "\n",
        "#visualize the images in dataset\n",
        "fig,axe=plt.subplots(1,5,figsize=(10,3))\n",
        "for i,ax in enumerate(axe):\n",
        "  ax.imshow(images.images[i],cmap='gray')\n",
        "  ax.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#normalize the input images\n",
        "X=X/0.16    # pixel value ranges from 0 to 16\n",
        "\n",
        "#one hot encoding for the ;abels\n",
        "encoder=OneHotEncoder(sparse_output=False)\n",
        "y_onehot = encoder.fit_transform(y.reshape(-1,1))\n",
        "\n",
        "#spliting the data for training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAaiMPFCKO84",
        "outputId": "b7b996d9-cc0d-4956-f219-4beadee174e6"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (1437, 64), Test shape: (360, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#implementing the neural network from scratch\n",
        "#initialization\n",
        "input_size=64\n",
        "hidden1_size=32\n",
        "hidden2_size=16\n",
        "output_size=10\n",
        "\n",
        "#Initialize the weights and bias\n",
        "\n",
        "W1 = np.random.randn(input_size,hidden1_size)*0.01\n",
        "B1 = np.zeros((1,hidden1_size))\n",
        "\n",
        "W2=np.random.randn(hidden1_size,hidden2_size)*0.01\n",
        "B2=np.zeros((1,hidden2_size))\n",
        "\n",
        "W3=np.random.randn(hidden2_size,output_size)*0.01\n",
        "B3=np.zeros((1,output_size))\n",
        "\n",
        "#Initializing Hyperparameters\n",
        "epoch=10000\n",
        "lr=0.01\n"
      ],
      "metadata": {
        "id": "WQiFd_qdLXzT"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activation fn and its derivative function\n",
        "\n",
        "def relu(z):\n",
        "  return np.maximum(0,z)\n",
        "def relu_derivative(z):\n",
        "  return (z>0).astype(float)\n",
        "\n",
        "def tanh(z):\n",
        "  return np.tanh(z)\n",
        "def tanh_derivative(z):\n",
        "  return 1-tanh(z)**2\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # stability\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "olHBN6mYRCA7"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define forward pass fn and the loss computing fn\n",
        "def forward(X):\n",
        "  z1= X @ W1 +B1\n",
        "  a1= relu(z1)\n",
        "\n",
        "  z2= a1 @ W2 + B2\n",
        "  a2= tanh(z2)\n",
        "\n",
        "  z3= a2 @ W3 +B3\n",
        "  y_pred=softmax(z3)\n",
        "\n",
        "  return z1,a1,z2,a2,z3,y_pred\n",
        "\n",
        "def loss(y_pred, y_true):\n",
        "    # Cross-entropy loss\n",
        "    m = y_true.shape[0]\n",
        "    loss = -np.sum(y_true * np.log(y_pred + 1e-8)) / m\n",
        "    return loss"
      ],
      "metadata": {
        "id": "2W1xc54XTW5r"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#forward and backward propagation\n",
        "for e in range(epoch):\n",
        "  z1,a1,z2,a2,z3,y_pred = forward(X_train)\n",
        "  compute_loss = loss(y_pred,y_train)\n",
        "\n",
        "  #Backpropagation\n",
        "  dLdz3 = (y_pred-y_train)/y_train.shape[0]\n",
        "\n",
        "  dLdW3 = a2.T @ dLdz3\n",
        "  dLdB3 = np.sum(dLdz3,axis=0,keepdims=True)\n",
        "\n",
        "  dLda2 = dLdz3 @ W3.T\n",
        "  dLdz2 = dLda2 * relu_derivative(z2)\n",
        "\n",
        "  dLdW2 = a1.T @ dLdz2\n",
        "  dLdB2 = np.sum(dLdz2,axis=0,keepdims=True)\n",
        "\n",
        "  dLda1 = dLdz2 @ W2.T\n",
        "  dLdz1 = dLda1 * tanh_derivative(z1)\n",
        "\n",
        "  dLdW1 = X_train.T @ dLdz1\n",
        "  dLdB1 = np.sum(dLdz1,axis=0,keepdims=True)\n",
        "\n",
        "  #Weights and Bias update\n",
        "  W1 -= lr * dLdW1\n",
        "  B1 -= lr * dLdB1\n",
        "  W2 -= lr * dLdW2\n",
        "  B2 -= lr * dLdB2\n",
        "  W3 -= lr * dLdW3\n",
        "  B3 -= lr * dLdB3\n",
        "\n",
        "\n",
        "  #printing the loss\n",
        "  if e%1000==0:\n",
        "       print(f\"Epoch:{e} Loss:{compute_loss}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrGH2BooS1NM",
        "outputId": "c1788932-7eb0-4f61-ee1f-300c656af69b"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:0 Loss:2.302302100178158\n",
            "Epoch:1000 Loss:1.6138165707543894\n",
            "Epoch:2000 Loss:1.1933788503695661\n",
            "Epoch:3000 Loss:0.9920419816650089\n",
            "Epoch:4000 Loss:0.8824447083258751\n",
            "Epoch:5000 Loss:0.76573002624355\n",
            "Epoch:6000 Loss:0.8508823269123226\n",
            "Epoch:7000 Loss:0.770762106999435\n",
            "Epoch:8000 Loss:0.7197946163567963\n",
            "Epoch:9000 Loss:0.6528054287889294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Forward pass on test data\n",
        "_, _, _, _, _, y_pred_test = forward(X_test)\n",
        "\n",
        "# Convert softmax output to class predictions\n",
        "predicted_classes = np.argmax(y_pred_test, axis=1)\n",
        "true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = np.mean(predicted_classes == true_classes)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVL1N5hCYjC6",
        "outputId": "8698abe0-5b8a-4db0-9a69-dec300557ca0"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 75.56%\n"
          ]
        }
      ]
    }
  ]
}